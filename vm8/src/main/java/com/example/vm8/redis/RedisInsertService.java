package com.example.vm8.redis;

import com.example.vm8.entity.TbDtfHrasAuto;
import com.example.vm8.entity.TbDtfHrasAutoPk;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import io.micrometer.core.annotation.Counted;
import io.micrometer.core.annotation.Timed;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.redis.core.RedisCallback;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Objects;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.IntStream;

@Slf4j
@Service
public class RedisInsertService {

    private static final String REDIS_KEY_PREFIX = "hras-data";

    private final StringRedisTemplate redisTemplate;

    private final ObjectMapper objectMapper;

    private static final Random RANDOM = new Random();

    @Value("${spring.properties.hibernate.jdbc.batch_size}")
    private int batchSize;

    @Value("${spring.application.vm-index}")
    private int vmIndex;

    private final Counter successCounter;

    private final Counter failureCounter;

    private final Timer timer;

    public RedisInsertService(StringRedisTemplate redisTemplate, ObjectMapper objectMapper, MeterRegistry meterRegistry) {
        this.redisTemplate = redisTemplate;
        this.objectMapper = objectMapper;
        this.successCounter = meterRegistry.counter("dummy_data.insert.success");
        this.failureCounter = meterRegistry.counter("dummy_data.insert.failure");
        this.timer = meterRegistry.timer("dummy_data.insert.timer");
    }

    @Timed(value = "dummy_data.insert.time", description = "Time taken to insert dummy data")
    @Counted(value = "dummy_data.insert.count", description = "Number of times dummy data is inserted")
    public void saveHrasDataInRedis(int batchIndex) {
        int baseIndex = ((vmIndex * 60) + batchIndex) * batchSize;
        String uniqueKey = "[VM-" + vmIndex + "] " + REDIS_KEY_PREFIX + ":" + System.currentTimeMillis();

        List<String> jsonRecords = generateJsonRecords(baseIndex, batchSize);

        if (jsonRecords.isEmpty()) {
            log.warn("No records to insert into Redis for key: {}", uniqueKey);
            return;
        }

        // ğŸš€ í•œ ë²ˆì˜ Pipelineì—ì„œ 10,000ê°œì”© ë¬¶ì–´ì„œ ì „ì†¡ (ìµœì í™”)
        int optimalBatchSize = Math.min(batchSize, 10000);
        AtomicInteger totalInserted = new AtomicInteger(); // ì‚½ì…ëœ ì´ ê°œìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì¦ê°€

        timer.record(() -> {
            try {
                redisTemplate.executePipelined((RedisCallback<Object>) connection -> {
                    for (int i = 0; i < jsonRecords.size(); i += optimalBatchSize) {
                        List<String> batch = jsonRecords.subList(i, Math.min(i + optimalBatchSize, jsonRecords.size()));
                        byte[][] values = batch.stream().map(String::getBytes).toArray(byte[][]::new);
                        connection.listCommands().rPush(uniqueKey.getBytes(), values); // ğŸš€ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ `RPUSH`
                        totalInserted.addAndGet(batch.size()); // ì„±ê³µì ìœ¼ë¡œ ì‚½ì…í•œ ê°œìˆ˜ ëˆ„ì 
                    }
                    return null;
                });
                successCounter.increment();
                log.info("âœ… Successfully inserted {} records into Redis (Key: {}). Total Success Count: {}", totalInserted.get(), uniqueKey, successCounter.count());

            } catch (Exception e) {
                failureCounter.increment();
                log.error("Failed to insert HRAS data into Redis", e);
            }
        });
    }


    /**
     * ì£¼ì–´ì§„ ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
     */
    private List<String> generateJsonRecords(int baseIndex, int batchSize) {
        return IntStream.range(0, batchSize)
                .mapToObj(i -> generateDummyRecordJson(baseIndex + i + 1))
                .filter(Objects::nonNull)  // JSON ë³€í™˜ ì‹¤íŒ¨í•œ ê²½ìš° ì œì™¸
                .toList();
    }

    /**
     * ë‹¨ì¼ ê°ì²´ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
     */
    private String generateDummyRecordJson(int csIdNumber) {
        try {
            TbDtfHrasAuto record = generateDummyRecord(csIdNumber);
            return objectMapper.writeValueAsString(record);
        } catch (JsonProcessingException e) {
            log.error("Failed to serialize HRAS data: CS_ID={}", csIdNumber, e);
            return null;  // ì‹¤íŒ¨í•œ ê²½ìš° ì œì™¸
        }
    }

    private TbDtfHrasAuto generateDummyRecord(int csIdNumber) {
        String csId = "CS_" + UUID.randomUUID().toString();
        long projectId = 1000000 + RANDOM.nextInt(100000); // PROJECT_ID: ëœë¤
        String name = "Project-" + csIdNumber; // NAME
        String riverName = "River-" + (RANDOM.nextInt(5) + 1); // RIVER_NAME: River-1 ~ River-5
        String riverReach = "Reach-" + (RANDOM.nextInt(10) + 1); // RIVER_REACH: Reach-1 ~ Reach-10
        long riverCode = 1000 + RANDOM.nextInt(100); // RIVER_CODE
        long wtlvVal = (RANDOM.nextLong() * 500); // WTLV_VAL: 0~500
        long flowVal = (RANDOM.nextLong() * 200); // FLOW_VAL: 0~200
        long velVal = RANDOM.nextLong() * 5; // VEL_VAL: 0~5
        LocalDateTime pdctDt = LocalDateTime.now().minusDays(RANDOM.nextInt(30)); // PDCT_DT: ìµœê·¼ 30ì¼ ë‚´ ë‚ ì§œ

        TbDtfHrasAutoPk pk = new TbDtfHrasAutoPk(csId, pdctDt);

        return TbDtfHrasAuto.builder()
                .pk(pk)
                .projectId(projectId)
                .name(name)
                .riverName(riverName)
                .riverReach(riverReach)
                .riverCode(riverCode)
                .wtlvVal(wtlvVal)
                .flowVal(flowVal)
                .velVal(velVal)
                .build();
    }
}

